{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e55c8fcd-71cd-470d-ada9-bbce0152ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0405 21:09:24.952613000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:26.987338000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:28.486340000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:31.298611000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:32.584272000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:34.148631000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip3 install --upgrade google-cloud-aiplatform\n",
    "!pip3 install --upgrade kfp\n",
    "!pip3 install --upgrade google-cloud-pipeline-components\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install pandas\n",
    "!pip3 install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f61dd-f7f2-4a2a-97dc-3c29d7360c82",
   "metadata": {},
   "source": [
    "### Good resources for custom components\n",
    "https://github.com/googleapis/python-aiplatform/tree/main/samples/model-builder\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples\n",
    "\n",
    "https://googleapis.dev/python/aiplatform/latest/aiplatform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e3a35-d9e5-4171-8cf7-d3ecca9cc44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48148fb-0cd6-4d57-b745-d57de9bb6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as vertex\n",
    "import kfp\n",
    "from kfp.v2.dsl import (component, Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics, Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda4133-2220-41f8-ad6f-cb0243887ff1",
   "metadata": {},
   "source": [
    "https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html\n",
    "\n",
    "https://pypi.org/project/google-cloud-aiplatform/\n",
    "\n",
    "https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4043be60-84f5-4aa5-92c3-80bbe2f58759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n",
      "Creating gs://test-fast/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'test-fast' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "      1552  2022-03-24T21:14:19Z  gs://test-fast/aiplatform-2022-03-24-21:14:19.320-aiplatform_custom_trainer_script-0.1.tar.gz#1648156459408486  metageneration=1\n",
      "      1774  2022-03-27T08:32:44Z  gs://test-fast/aiplatform-2022-03-27-11:32:43.707-aiplatform_custom_trainer_script-0.1.tar.gz#1648369964419591  metageneration=1\n",
      "        60  2021-11-14T15:38:56Z  gs://test-fast/batch_test.csv#1636904336323978  metageneration=1\n",
      "        52  2021-11-14T16:14:44Z  gs://test-fast/batch_test1.csv#1636906484597636  metageneration=1\n",
      "      4551  2022-04-05T20:08:54Z  gs://test-fast/finalized_model.sav#1649189334168342  metageneration=1\n",
      "   2105688  2022-04-05T19:53:51Z  gs://test-fast/model#1649188431248263  metageneration=1\n",
      "   2105848  2022-04-05T19:40:15Z  gs://test-fast/storage-object-name#1649187615058728  metageneration=1\n",
      "                                 gs://test-fast/141610882258/\n",
      "                                 gs://test-fast/aiplatform-custom-training-2022-03-24-21:14:19.437/\n",
      "                                 gs://test-fast/aiplatform-custom-training-2022-03-27-11:32:44.390/\n",
      "                                 gs://test-fast/census/\n",
      "                                 gs://test-fast/data/\n",
      "                                 gs://test-fast/executor_files/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2021_11_14T07_54_09_554Z/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2021_11_14T08_41_08_653Z/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2021_11_14T11_37_18_583Z/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2022_02_21T00_25_18_548Z/\n",
      "TOTAL: 7 objects, 4219525 bytes (4.02 MiB)\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"\" \n",
    "PROJECT_NUMBER = \"\" \n",
    "REGION = \"us-central1\"  \n",
    "BUCKET_NAME = \"\"  \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "!gsutil mb -l $REGION $BUCKET_URI\n",
    "!gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34c6144-e2e9-450e-a93c-397aa5228572",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project=,\n",
    "\n",
    "    # the Vertex AI region you will use\n",
    "    # defaults to us-central1\n",
    "    location=REGION,\n",
    "\n",
    "    # Google Cloud Storage bucket in same region as location\n",
    "    # used to stage artifacts\n",
    "    staging_bucket=BUCKET_URI,\n",
    "\n",
    "    # the name of the experiment to use to track\n",
    "    # logged metrics and parameters\n",
    "    experiment=,\n",
    "\n",
    "    # description of the experiment above\n",
    "    experiment_description='my experiment description'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7a6d4-7503-4308-97dc-65db97729a72",
   "metadata": {},
   "source": [
    "https://pypi.org/project/google-cloud-aiplatform/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d3479-d4ce-4484-baa6-d428d819f6ae",
   "metadata": {},
   "source": [
    "# AutoML training job\n",
    "AutoML can be used to automatically train a wide variety of image model types. AutoML automates the following:\n",
    "\n",
    "* Dataset preprocessing\n",
    "* Feature Engineering\n",
    "* Data feeding\n",
    "* Model Architecture selection\n",
    "* Hyperparameter tuning\n",
    "* Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b469683f-deed-4619-a37c-54ddc3a859ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4169501160.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [361]\u001b[0;36m\u001b[0m\n\u001b[0;31m    @component()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "@component()\n",
    "def create_dataset():\n",
    "    \n",
    "@component()\n",
    "def train():\n",
    "    \n",
    "@component()\n",
    "def evaluate():\n",
    "    \n",
    "@component()\n",
    "def deploy():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed640c7-54d3-4a3c-90d9-beaa68cbb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def create_tabular_dataset() -> str:\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "\n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    bq_source = f'bq://kubeflow-demos.flowers.iris'\n",
    "    \n",
    "    #TODO: Missing dataset creation\n",
    "    \n",
    "    dataset.wait()\n",
    "\n",
    "    print(f'\\tDataset: \"{dataset.display_name}\"')\n",
    "    print(f'\\tname: \"{dataset.resource_name}\"')\n",
    "    \n",
    "    return dataset.resource_name\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def train_autoML_tabular(dataset_resource: str) -> str:\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "\n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    job = vertex.AutoMLTabularTrainingJob(\n",
    "                                        display_name=\"flowers\",\n",
    "                                        optimization_prediction_type=\"classification\",\n",
    "                                        column_specs={\"petal_length\": \"auto\",\n",
    "                                                      \"petal_width\": \"auto\",\n",
    "                                                      \"sepal_length\": \"auto\",\n",
    "                                                      \"sepal_width\": \"auto\",\n",
    "                                                      \"species\": \"auto\"})\n",
    "\n",
    "    print(job)\n",
    "\n",
    "    #TODO: missing stuff\n",
    "    flower_dataset = \n",
    "\n",
    "    model = job.run(dataset=flower_dataset,\n",
    "                    target_column=\"species\")\n",
    "    \n",
    "    model.wait()\n",
    "    \n",
    "    print(model.resource_name)\n",
    "    \n",
    "    return model.resource_name\n",
    "    \n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-storage\", \"catboost\", \"sklearn\"])\n",
    "def train_catboost():\n",
    "    print(\"train\")\n",
    "    import sklearn\n",
    "    from sklearn import datasets\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "\n",
    "    import catboost\n",
    "    \n",
    "    #TODO: Train catboost model\n",
    "\n",
    "    from google.cloud import storage\n",
    "\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"test-fast\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = FILE_PATH\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = FILE_PATH\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, \"gs://\" + destination_blob_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "#TODO: missing\n",
    "@component()\n",
    "def train_scikit():\n",
    "    print(\"train\")\n",
    "    import sklearn\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn import datasets\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "\n",
    "    model = SVC()\n",
    "    model.fit(iris.data, iris.target)\n",
    "    \n",
    "    import pickle\n",
    "    # save the model to disk\n",
    "    FILE_PATH = 'finalized_model.sav'\n",
    "    pickle.dump(model, open(FILE_PATH, 'wb'))\n",
    "    \n",
    "    from google.cloud import storage\n",
    "\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"test-fast\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = FILE_PATH\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = FILE_PATH\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, \"gs://\" + destination_blob_name\n",
    "        )\n",
    "    )\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def evaluate(model_resource: str) -> str:\n",
    "    print(\"evaluate\")\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "\n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                    location='us-central1',\n",
    "                    staging_bucket='gs://test-fast/',\n",
    "                    experiment='my-experiment',\n",
    "                    experiment_description='my experiment description')\n",
    "    \n",
    "    #TODO: get model\n",
    "    # Get model resource ID\n",
    "    models = \n",
    "\n",
    "    # Get a reference to the Model Service client\n",
    "    client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "    model_service_client = vertex.gapic.ModelServiceClient(client_options=client_options)\n",
    "\n",
    "    model_evaluations = model_service_client.list_model_evaluations(\n",
    "        parent=model_resource\n",
    "    )\n",
    "    model_evaluation = list(model_evaluations)[0]\n",
    "    \n",
    "    print(model_evaluation)\n",
    "    \n",
    "    return model_resource\n",
    "    \n",
    "#TODO: missing\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def create_endpoint_deploy_model() -> str:\n",
    "    print(\"Endpoint\")\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "    \n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    endpoint = vertex.Endpoint.create(display_name=\"opti\")\n",
    "\n",
    "    model = vertex.Model(model_resource)\n",
    "\n",
    "    #endpoint.deploy(model=model)\n",
    "    #endpoint.resource_name\n",
    "    model.deploy()\n",
    "    \n",
    "    return \"test\"\n",
    "\n",
    "#TODO: missing\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def batch_prediction() -> str:\n",
    "    print(\"Batch Prediction\")\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "    \n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    model = vertex.Model(model_resource)\n",
    "    #https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model\n",
    "    #https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/migration/UJ4%20Vertex%20SDK%20AutoML%20Tabular%20Binary%20Classification.ipynb\n",
    "    batch_predict_job = model.batch_predict(\n",
    "                                job_display_name=\"opti_\",\n",
    "                                bigquery_source = f'bq://kubeflow-demos.flowers.iris',\n",
    "                                bigquery_destination_prefix = f'bq://kubeflow-demos.flowers',\n",
    "                                sync=True)\n",
    "\n",
    "    print(batch_predict_job)\n",
    "    \n",
    "    return \"batch is done\"\n",
    "\n",
    "@component\n",
    "def print_op(message: str):\n",
    "    \"\"\"Prints a message.\"\"\"\n",
    "    print(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fddb45-d074-4043-8ea1-6cf2f1eb4bee",
   "metadata": {},
   "source": [
    "https://www.kubeflow.org/docs/components/pipelines/sdk-v2/v2-component-io/\n",
    "\n",
    "https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.AutoMLTabularTrainingJob\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/training/automl-api\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/migration/UJ4%20Vertex%20SDK%20AutoML%20Tabular%20Binary%20Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e6cfbe-591a-4b6c-87f6-3475de4dd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-opti\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    bucket: str = BUCKET_URI,\n",
    "    baseline_accuracy: float = 70.0\n",
    "):\n",
    "    create_tabular_dataset_task = create_tabular_dataset()\n",
    "    create_tabular_dataset_task.set_caching_options(True)\n",
    "    \n",
    "    train_autoML_task = train_autoML_tabular(create_tabular_dataset_task.output)\n",
    "    train_autoML_task.set_caching_options(True)\n",
    "    \n",
    "    train_catboost_task = train_catboost()\n",
    "    train_catboost_task.set_caching_options(True)\n",
    "    train_catboost_task.after(create_tabular_dataset_task)\n",
    "    \n",
    "    train_scikit_task = train_scikit()\n",
    "    train_scikit_task.set_caching_options(False)\n",
    "    train_scikit_task.after(create_tabular_dataset_task)\n",
    "    \n",
    "    evaluate_task = evaluate()\n",
    "    evaluate_task.set_caching_options(True)\n",
    "    evaluate_task.after(train_scikit_task)\n",
    "    evaluate_task.after(train_catboost_task)\n",
    "    \n",
    "    batch_prediction_task = batch_prediction(evaluate_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14ac497-112e-44f4-943e-72a9b241bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yarkoni/projects/workshop/venv/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1278: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88e76ae0-3171-4e4d-ae29-220a0f14dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220428113336\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220428113336')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-opti-20220428113336?project=141610882258\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220428113336\n"
     ]
    }
   ],
   "source": [
    "# Instantiate PipelineJob object\n",
    "vertex_pipeline_job = ai_magic.PipelineJob(\n",
    "    display_name=\"test-opti\",\n",
    "\n",
    "    # Whether or not to enable caching\n",
    "    # True = always cache pipeline step result\n",
    "    # False = never cache pipeline step result\n",
    "    # None = defer to cache option for each pipeline component in the pipeline definition\n",
    "    enable_caching=True,\n",
    "\n",
    "    # Local or GCS path to a compiled pipeline definition\n",
    "    template_path=\"pipeline-dag.json\",\n",
    "\n",
    "    # Dictionary containing input parameters for your pipeline\n",
    "    parameter_values={\"sdffsdfds\":\"sdfsdf\"},\n",
    "\n",
    "    # GCS path to act as the pipeline root\n",
    "    pipeline_root=BUCKET_URI,\n",
    ")\n",
    "\n",
    "# Execute pipeline in Vertex AI and monitor until completion\n",
    "ai_magic_false.run(\n",
    "  # Email address of service account to use for the pipeline run\n",
    "  # You must have iam.serviceAccounts.actAs permission on the service account to use it\n",
    "  #service_account=service_account,\n",
    "\n",
    "  # Whether this function call should be synchronous (wait for pipeline run to finish before terminating)\n",
    "  # or asynchronous (return immediately)\n",
    "  sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "79bdbf98-57bc-4c17-9671-1d0832e571e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 13:49:19.734209000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "jobs = vertex.PipelineJob.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82858c6c-ebd7-4cef-b29a-3ae2665be848",
   "metadata": {},
   "source": [
    "### Or simply use the premade components \n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_automl_images.ipynb\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f47b3-b11c-4855-9a20-638dbed91ff9",
   "metadata": {},
   "source": [
    "### Adding batch capabilities\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/automl/showcase_automl_image_classification_batch.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

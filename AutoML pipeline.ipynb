{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e55c8fcd-71cd-470d-ada9-bbce0152ac98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0405 21:09:24.952613000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:26.987338000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:28.486340000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:31.298611000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:32.584272000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0405 21:09:34.148631000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip3 install --upgrade google-cloud-aiplatform\n",
    "!pip3 install --upgrade kfp\n",
    "!pip3 install --upgrade google-cloud-pipeline-components\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install pandas\n",
    "!pip3 install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61f61dd-f7f2-4a2a-97dc-3c29d7360c82",
   "metadata": {},
   "source": [
    "### Good resources for custom components\n",
    "https://github.com/googleapis/python-aiplatform/tree/main/samples/model-builder\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples\n",
    "\n",
    "https://googleapis.dev/python/aiplatform/latest/aiplatform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e3a35-d9e5-4171-8cf7-d3ecca9cc44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b48148fb-0cd6-4d57-b745-d57de9bb6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as vertex\n",
    "import kfp\n",
    "from kfp.v2.dsl import (component, Artifact, Dataset, Input, InputPath, Model, Output, OutputPath, ClassificationMetrics, Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda4133-2220-41f8-ad6f-cb0243887ff1",
   "metadata": {},
   "source": [
    "https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.dsl.html\n",
    "\n",
    "https://pypi.org/project/google-cloud-aiplatform/\n",
    "\n",
    "https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "4043be60-84f5-4aa5-92c3-80bbe2f58759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 13:56:23.661062000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 13:56:25.180716000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://test-fast/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'test-fast' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 13:56:27.791241000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1552  2022-03-24T21:14:19Z  gs://test-fast/aiplatform-2022-03-24-21:14:19.320-aiplatform_custom_trainer_script-0.1.tar.gz#1648156459408486  metageneration=1\n",
      "      1774  2022-03-27T08:32:44Z  gs://test-fast/aiplatform-2022-03-27-11:32:43.707-aiplatform_custom_trainer_script-0.1.tar.gz#1648369964419591  metageneration=1\n",
      "        60  2021-11-14T15:38:56Z  gs://test-fast/batch_test.csv#1636904336323978  metageneration=1\n",
      "        52  2021-11-14T16:14:44Z  gs://test-fast/batch_test1.csv#1636906484597636  metageneration=1\n",
      "      4551  2022-04-05T20:08:54Z  gs://test-fast/finalized_model.sav#1649189334168342  metageneration=1\n",
      "   2105688  2022-04-05T19:53:51Z  gs://test-fast/model#1649188431248263  metageneration=1\n",
      "   2105848  2022-04-05T19:40:15Z  gs://test-fast/storage-object-name#1649187615058728  metageneration=1\n",
      "                                 gs://test-fast/141610882258/\n",
      "                                 gs://test-fast/aiplatform-custom-training-2022-03-24-21:14:19.437/\n",
      "                                 gs://test-fast/aiplatform-custom-training-2022-03-27-11:32:44.390/\n",
      "                                 gs://test-fast/census/\n",
      "                                 gs://test-fast/data/\n",
      "                                 gs://test-fast/executor_files/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2021_11_14T07_54_09_554Z/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2021_11_14T08_41_08_653Z/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2021_11_14T11_37_18_583Z/\n",
      "                                 gs://test-fast/prediction-fast-test-12-2022_02_21T00_25_18_548Z/\n",
      "TOTAL: 7 objects, 4219525 bytes (4.02 MiB)\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"kubeflow-demos\" \n",
    "PROJECT_NUMBER = \"141610882258\" \n",
    "REGION = \"us-central1\"  \n",
    "BUCKET_NAME = \"test-fast\"  \n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "!gsutil mb -l $REGION $BUCKET_URI\n",
    "!gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "e34c6144-e2e9-450e-a93c-397aa5228572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 13:56:32.264755000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "E0407 13:56:34.273604000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "vertex.init(\n",
    "    # your Google Cloud Project ID or number\n",
    "    # environment default used is not set\n",
    "    project=PROJECT_ID,\n",
    "\n",
    "    # the Vertex AI region you will use\n",
    "    # defaults to us-central1\n",
    "    location=REGION,\n",
    "\n",
    "    # Google Cloud Storage bucket in same region as location\n",
    "    # used to stage artifacts\n",
    "    staging_bucket=BUCKET_URI,\n",
    "\n",
    "    # the name of the experiment to use to track\n",
    "    # logged metrics and parameters\n",
    "    experiment='my-experiment',\n",
    "\n",
    "    # description of the experiment above\n",
    "    experiment_description='my experiment description'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7a6d4-7503-4308-97dc-65db97729a72",
   "metadata": {},
   "source": [
    "https://pypi.org/project/google-cloud-aiplatform/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d3479-d4ce-4484-baa6-d428d819f6ae",
   "metadata": {},
   "source": [
    "# AutoML training job\n",
    "AutoML can be used to automatically train a wide variety of image model types. AutoML automates the following:\n",
    "\n",
    "* Dataset preprocessing\n",
    "* Feature Engineering\n",
    "* Data feeding\n",
    "* Model Architecture selection\n",
    "* Hyperparameter tuning\n",
    "* Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b469683f-deed-4619-a37c-54ddc3a859ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4169501160.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [361]\u001b[0;36m\u001b[0m\n\u001b[0;31m    @component()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "@component()\n",
    "def create_dataset():\n",
    "    \n",
    "@component()\n",
    "def train():\n",
    "    \n",
    "@component()\n",
    "def evaluate():\n",
    "    \n",
    "@component()\n",
    "def deploy():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "6ed640c7-54d3-4a3c-90d9-beaa68cbb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def create_tabular_dataset() -> str:\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "\n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    bq_source = f'bq://kubeflow-demos.flowers.iris'\n",
    "\n",
    "    dataset = vertex.TabularDataset.create(display_name='iris', bq_source=bq_source,)\n",
    "\n",
    "    dataset.wait()\n",
    "\n",
    "    print(f'\\tDataset: \"{dataset.display_name}\"')\n",
    "    print(f'\\tname: \"{dataset.resource_name}\"')\n",
    "    \n",
    "    return dataset.resource_name\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def train_autoML_tabular(dataset_resource: str) -> str:\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "\n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    job = vertex.AutoMLTabularTrainingJob(\n",
    "                                        display_name=\"flowers\",\n",
    "                                        optimization_prediction_type=\"classification\",\n",
    "                                        column_specs={\"petal_length\": \"auto\",\n",
    "                                                      \"petal_width\": \"auto\",\n",
    "                                                      \"sepal_length\": \"auto\",\n",
    "                                                      \"sepal_width\": \"auto\",\n",
    "                                                      \"species\": \"auto\"})\n",
    "\n",
    "    print(job)\n",
    "\n",
    "    flower_dataset = vertex.TabularDataset(dataset_resource)\n",
    "\n",
    "    model = job.run(dataset=flower_dataset,\n",
    "                    model_display_name=\"flowers\",\n",
    "                    training_fraction_split=0.8,\n",
    "                    validation_fraction_split=0.1,\n",
    "                    test_fraction_split=0.1,\n",
    "                    budget_milli_node_hours=1000,\n",
    "                    disable_early_stopping=False,\n",
    "                    target_column=\"species\")\n",
    "    \n",
    "    model.wait()\n",
    "    \n",
    "    print(model.resource_name)\n",
    "    \n",
    "    return model.resource_name\n",
    "    \n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-storage\", \"catboost\", \"sklearn\"])\n",
    "def train_catboost():\n",
    "    print(\"train\")\n",
    "    import sklearn\n",
    "    from sklearn import datasets\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "\n",
    "    import catboost\n",
    "    model = catboost.CatBoostClassifier(loss_function='MultiClass')\n",
    "\n",
    "    model.fit(iris.data, iris.target)\n",
    "    FILE_PATH = \"model\"\n",
    "    model.save_model(FILE_PATH)\n",
    "\n",
    "    from_file = catboost.CatBoostClassifier()\n",
    "\n",
    "    from google.cloud import storage\n",
    "\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"test-fast\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = FILE_PATH\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = FILE_PATH\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, \"gs://\" + destination_blob_name\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-storage\", \"sklearn\"])\n",
    "def train_scikit():\n",
    "    print(\"train\")\n",
    "    import sklearn\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn import datasets\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "\n",
    "    model = SVC()\n",
    "    model.fit(iris.data, iris.target)\n",
    "    \n",
    "    import pickle\n",
    "    # save the model to disk\n",
    "    FILE_PATH = 'finalized_model.sav'\n",
    "    pickle.dump(model, open(FILE_PATH, 'wb'))\n",
    "    \n",
    "    from google.cloud import storage\n",
    "\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    bucket_name = \"test-fast\"\n",
    "    # The path to your file to upload\n",
    "    source_file_name = FILE_PATH\n",
    "    # The ID of your GCS object\n",
    "    destination_blob_name = FILE_PATH\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, \"gs://\" + destination_blob_name\n",
    "        )\n",
    "    )\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def evaluate(model_resource: str) -> str:\n",
    "    print(\"evaluate\")\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "\n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                    location='us-central1',\n",
    "                    staging_bucket='gs://test-fast/',\n",
    "                    experiment='my-experiment',\n",
    "                    experiment_description='my experiment description')\n",
    "    \n",
    "    # Get model resource ID\n",
    "    models = vertex.Model(model_resource)\n",
    "\n",
    "    # Get a reference to the Model Service client\n",
    "    client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
    "    model_service_client = vertex.gapic.ModelServiceClient(client_options=client_options)\n",
    "\n",
    "    model_evaluations = model_service_client.list_model_evaluations(\n",
    "        parent=model_resource\n",
    "    )\n",
    "    model_evaluation = list(model_evaluations)[0]\n",
    "    \n",
    "    print(model_evaluation)\n",
    "    \n",
    "    return model_resource\n",
    "    \n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def create_endpoint_deploy_model(model_resource: str) -> str:\n",
    "    print(\"Endpoint\")\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "    \n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    endpoint = vertex.Endpoint.create(display_name=\"opti\")\n",
    "\n",
    "    model = vertex.Model(model_resource)\n",
    "\n",
    "    #endpoint.deploy(model=model)\n",
    "    #endpoint.resource_name\n",
    "    model.deploy()\n",
    "    \n",
    "    return \"test\"\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def batch_prediction(model_resource: str) -> str:\n",
    "    print(\"Batch Prediction\")\n",
    "    \n",
    "    import google.cloud.aiplatform as vertex\n",
    "    \n",
    "    vertex.init(project='kubeflow-demos',\n",
    "                location='us-central1',\n",
    "                staging_bucket='gs://test-fast/',\n",
    "                experiment='my-experiment',\n",
    "                experiment_description='my experiment description')\n",
    "    \n",
    "    model = vertex.Model(model_resource)\n",
    "    #https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model\n",
    "    #https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/migration/UJ4%20Vertex%20SDK%20AutoML%20Tabular%20Binary%20Classification.ipynb\n",
    "    batch_predict_job = model.batch_predict(\n",
    "                                job_display_name=\"opti_\",\n",
    "                                bigquery_source = f'bq://kubeflow-demos.flowers.iris',\n",
    "                                bigquery_destination_prefix = f'bq://kubeflow-demos.flowers',\n",
    "                                sync=True)\n",
    "\n",
    "    print(batch_predict_job)\n",
    "    \n",
    "    return \"batch is done\"\n",
    "\n",
    "@component\n",
    "def print_op(message: str):\n",
    "    \"\"\"Prints a message.\"\"\"\n",
    "    print(message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fddb45-d074-4043-8ea1-6cf2f1eb4bee",
   "metadata": {},
   "source": [
    "https://www.kubeflow.org/docs/components/pipelines/sdk-v2/v2-component-io/\n",
    "\n",
    "https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.AutoMLTabularTrainingJob\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/training/automl-api\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/migration/UJ4%20Vertex%20SDK%20AutoML%20Tabular%20Binary%20Classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "72e6cfbe-591a-4b6c-87f6-3475de4dd786",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"train-opti\")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    bucket: str = BUCKET_URI,\n",
    "    baseline_accuracy: float = 70.0\n",
    "):\n",
    "    create_tabular_dataset_task = create_tabular_dataset()\n",
    "    create_tabular_dataset_task.set_caching_options(True)\n",
    "    \n",
    "    train_autoML_task = train_autoML_tabular(create_tabular_dataset_task.output)\n",
    "    train_autoML_task.set_caching_options(True)\n",
    "    \n",
    "    train_catboost_task = train_catboost()\n",
    "    train_catboost_task.set_caching_options(True)\n",
    "    train_catboost_task.after(create_tabular_dataset_task)\n",
    "    \n",
    "    train_scikit_task = train_scikit()\n",
    "    train_scikit_task.set_caching_options(True)\n",
    "    train_scikit_task.after(create_tabular_dataset_task)\n",
    "    \n",
    "    evaluate_task = evaluate(train_autoML_task.output)\n",
    "    evaluate_task.set_caching_options(True)\n",
    "    evaluate_task.after(train_scikit_task)\n",
    "    evaluate_task.after(train_catboost_task)\n",
    "    \n",
    "    batch_prediction_task = batch_prediction(evaluate_task.output)\n",
    "    \n",
    "    exit_task = print_op(message=)\n",
    "\n",
    "    with kfp.dsl.ExitHandler(exit_task):\n",
    "        print_op(message=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "e14ac497-112e-44f4-943e-72a9b241bead",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only one global exit_handler is allowed and all ops need to be included.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [401]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkfp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compiler\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpipeline-dag.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/workshop/venv/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1289\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, pipeline_func, package_path, pipeline_name, pipeline_parameters, type_check)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check\n\u001b[1;32m   1288\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mCOMPILING_FOR_V2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m     pipeline_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_pipeline_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_parameters_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_pipeline(pipeline_job, package_path)\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/workshop/venv/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1216\u001b[0m, in \u001b[0;36mCompiler._create_pipeline_v2\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_parameters_override)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mops:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is missing from pipeline.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_exit_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsl_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_and_inject_artifact(dsl_pipeline)\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;66;03m# Fill in the default values.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/workshop/venv/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1111\u001b[0m, in \u001b[0;36mCompiler._validate_exit_handler\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39mgroups:\n\u001b[1;32m   1108\u001b[0m         _validate_exit_handler_helper(g, exiting_op_names,\n\u001b[1;32m   1109\u001b[0m                                       handler_exists)\n\u001b[0;32m-> 1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_exit_handler_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/workshop/venv/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1108\u001b[0m, in \u001b[0;36mCompiler._validate_exit_handler.<locals>._validate_exit_handler_helper\u001b[0;34m(group, exiting_op_names, handler_exists)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     exiting_op_names\u001b[38;5;241m.\u001b[39mextend([x\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39mops])\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39mgroups:\n\u001b[0;32m-> 1108\u001b[0m     \u001b[43m_validate_exit_handler_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexiting_op_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mhandler_exists\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/workshop/venv/lib/python3.9/site-packages/kfp/v2/compiler/compiler.py:1099\u001b[0m, in \u001b[0;36mCompiler._validate_exit_handler.<locals>._validate_exit_handler_helper\u001b[0;34m(group, exiting_op_names, handler_exists)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit_handler\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handler_exists \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exiting_op_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1100\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly one global exit_handler is allowed and all ops need to be included.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1101\u001b[0m         )\n\u001b[1;32m   1102\u001b[0m     handler_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group\u001b[38;5;241m.\u001b[39mops:\n",
      "\u001b[0;31mValueError\u001b[0m: Only one global exit_handler is allowed and all ops need to be included."
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipeline-dag.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "88e76ae0-3171-4e4d-ae29-220a0f14dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 15:25:50.996754000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/train-opti-20220407152552?project=141610882258\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407141741 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/141610882258/locations/us-central1/pipelineJobs/train-opti-20220407152552 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# Instantiate PipelineJob object\n",
    "vertex_pipeline_job = vertex.PipelineJob(\n",
    "    display_name=\"test-opti\",\n",
    "\n",
    "    # Whether or not to enable caching\n",
    "    # True = always cache pipeline step result\n",
    "    # False = never cache pipeline step result\n",
    "    # None = defer to cache option for each pipeline component in the pipeline definition\n",
    "    enable_caching=True,\n",
    "\n",
    "    # Local or GCS path to a compiled pipeline definition\n",
    "    template_path=\"pipeline-dag.json\",\n",
    "\n",
    "    # Dictionary containing input parameters for your pipeline\n",
    "    parameter_values={},\n",
    "\n",
    "    # GCS path to act as the pipeline root\n",
    "    pipeline_root=BUCKET_URI,\n",
    ")\n",
    "\n",
    "# Execute pipeline in Vertex AI and monitor until completion\n",
    "vertex_pipeline_job.run(\n",
    "  # Email address of service account to use for the pipeline run\n",
    "  # You must have iam.serviceAccounts.actAs permission on the service account to use it\n",
    "  #service_account=service_account,\n",
    "\n",
    "  # Whether this function call should be synchronous (wait for pipeline run to finish before terminating)\n",
    "  # or asynchronous (return immediately)\n",
    "  sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "79bdbf98-57bc-4c17-9671-1d0832e571e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0407 13:49:19.734209000 4753593856 fork_posix.cc:76]                  Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "jobs = vertex.PipelineJob.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82858c6c-ebd7-4cef-b29a-3ae2665be848",
   "metadata": {},
   "source": [
    "### Or simply use the premade components \n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_automl_images.ipynb\n",
    "\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f47b3-b11c-4855-9a20-638dbed91ff9",
   "metadata": {},
   "source": [
    "### Adding batch capabilities\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/automl/showcase_automl_image_classification_batch.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
